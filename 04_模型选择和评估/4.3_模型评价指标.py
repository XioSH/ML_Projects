


# 4.3.1 分类效果评价
## 混淆矩阵：每一行代表预测值，每一列代表的是实际的类别
## 精度: 表示正确分类的样本比例
## 精确度: 也可以称为查准率，它表示的是预测为正的样本中有多少是真正的正样本
## 召回率: 表示的是样本中的正例有多少被预测正确了
## F1 Score: 是一种综合评价指标，是精确率和召回率两个值的调和平均，用来反映模型的整体情况
## ROC曲线和AUC值: 很多分类器为了测试样本会产生一个实值或者概率预测值，然后将这个预测值与一个分类阈值进行比较，如果大于阈值则分为正类，否则为反类。
##                 阈值的好坏直接反映了学习算法的泛化能力。根据预测值的概率，可以使用受试者工作特征曲线（ROC）来分析机器学习算法的泛化能力。在ROC曲线中，纵轴是真正例率（True Positive Rate）​，横轴是假正例率（False Positive Rate）​
##                 ROC曲线与横轴围成的面积大小称为学习器的AUC（Area Under roc Curve）​，该值越接近于1，说明算法模型越好

# 4.3.2 回归效果评价
## 显著性检验
## R²和调整后的R²
## AIC和BIC
## 系统显著性检验
## D.W检验
## 条件数

# 4.3.3 聚类效果评价
## 有真实标签的聚类结果评价方法（同质性、完整性、V测度等）
## 无真实标签的聚类结果评价方法（轮廓系数、CH指数、DB指数等）